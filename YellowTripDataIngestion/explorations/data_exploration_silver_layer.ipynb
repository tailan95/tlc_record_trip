{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8241fa1e-430c-413d-b227-0fccc9a62b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploração de Dados – Camada Silver\n",
    "\n",
    "Este notebook explora os registros de viagens limpos e validados armazenados na **Camada Silver**.  \n",
    "O objetivo é avaliar a eficácia das transformações aplicadas, incluindo o tratamento de valores ausentes, regras de consistência, verificações de plausibilidade e remoção de duplicatas.  \n",
    "\n",
    "Enquanto a Camada Bronze contém dados brutos ingeridos, a Camada Silver garante que os registros estejam padronizados, verificados quanto à qualidade e prontos para análises avançadas e agregações na Camada Gold.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d525cd-f2a0-4ad3-8114-04868dce76f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Etapa 1 – Carregar Dados da Camada Silver\n",
    "\n",
    "A tabela Silver é carregada em um DataFrame do Spark para validar os resultados das regras de limpeza e transformação aplicadas.  \n",
    "Os primeiros registros são visualizados para confirmar a consistência.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8243ad-e90e-4447-ba09-0d8aed6db4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.sql(\"SELECT * FROM tlc.tripdata.tripdata_silver\")\n",
    "df_silver.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcbc541-01d5-4894-8be7-0b07474fa89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Etapa 2 – Validação do Esquema da Camada Silver\n",
    "\n",
    "Na Camada Silver, o esquema é validado em relação ao **dicionário de dados oficial da TLC** para garantir tipagem e consistência adequadas.  \n",
    "O objetivo é confirmar que todas as transformações aplicadas aos dados da Bronze preservaram ou corrigiram o esquema esperado.  \n",
    "\n",
    "- **Campos de timestamp** (ex.: `tpep_pickup_datetime`, `tpep_dropoff_datetime`) são corretamente reconhecidos como `timestamp`.  \n",
    "- **Colunas numéricas** (ex.: `trip_distance`, `fare_amount`, `total_amount`, `passenger_count`) são convertidas para os tipos numéricos corretos (`int`, `double`).  \n",
    "- **Códigos categóricos** (ex.: `VendorID`, `RatecodeID`, `payment_type`) são padronizados como `int`.  \n",
    "- Valores ausentes ou inválidos recebem códigos padrão quando aplicável (ex.: `RatecodeID = 99`, `payment_type = 5`).  \n",
    "\n",
    "Isso garante que a Camada Silver forneça um **esquema limpo e consistente** para o processamento subsequente na Camada Gold.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9b758f-488e-48a4-be9a-4894bcf8140e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268db788-c091-4a71-b0f1-46eaebb872a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Etapa 3 – Valores Ausentes na Camada Silver\n",
    "\n",
    "Na Camada Silver, os valores ausentes são cuidadosamente tratados para melhorar a qualidade dos dados.  \n",
    "\n",
    "Um resumo dos valores nulos restantes por coluna é gerado para confirmar que apenas **campos não críticos** podem ainda conter dados ausentes na Camada Silver.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3a042e-e9b3-47bc-8528-43e627a4e9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, to_date, lit\n",
    "missing_summary = df_silver.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_silver.columns])\n",
    "missing_summary.display()\n",
    "print(f\"The dataset contains {df_silver.count()} values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f783a7c-bc28-4326-a652-25702bdcb072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Etapa 5 – Verificações de Consistência da Camada Silver\n",
    "\n",
    "As seguintes regras são aplicadas para garantir consistência lógica e de domínio:  \n",
    "\n",
    "- **Consistência de início dos dados**: As datas devem começar após jan-2023.  \n",
    "- **Consistência temporal**: O horário de desembarque deve ser sempre maior que o horário de embarque.  \n",
    "- **Restrições de passageiros**: A contagem de passageiros deve estar entre 1 e 7.  \n",
    "- **Distância da viagem**: Deve ser estritamente positiva.  \n",
    "- **Validade financeira**: Campos como `fare_amount`, `total_amount`, `tip_amount` e outros componentes de custo devem ser não negativos, e `total_amount` deve ser maior que zero.  \n",
    "\n",
    "Essas verificações refinam o conjunto de dados, removendo registros incompletos, implausíveis ou contraditórios antes de avançar para a Camada Gold.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7555dca0-42e4-47f8-b19c-397dd185f7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min as spark_min, max as spark_max\n",
    "df_silver.agg(\n",
    "    spark_min(col(\"tpep_pickup_datetime\")).alias(\"initial_pickup_datetime\"),\n",
    "    spark_max(col(\"tpep_pickup_datetime\")).alias(\"final_pickup_datetime\")\n",
    ").display()\n",
    "lower_values_in = df_silver.filter(col(\"tpep_pickup_datetime\") < to_date(lit(\"2023-01-01\")))\n",
    "lower_values_off = df_silver.filter(col(\"tpep_dropoff_datetime\") < to_date(lit(\"2023-01-01\")))\n",
    "dropin_ = df_silver.filter(col(\"tpep_dropoff_datetime\") < col(\"tpep_pickup_datetime\")) \n",
    "passenger_ = df_silver.filter((col(\"passenger_count\") <= 0) | (col(\"passenger_count\") > 7)) \n",
    "tistances_ = df_silver.filter(col(\"trip_distance\") <= 0) \n",
    "fate_ = df_silver.filter(col(\"fare_amount\") < 0)\n",
    "extra_ = df_silver.filter(col(\"extra\") < 0)\n",
    "mta_tax_ = df_silver.filter(col(\"mta_tax\") < 0)\n",
    "mta_tax_ = df_silver.filter(col(\"mta_tax\") < 0)\n",
    "tips_ = df_silver.filter(col(\"tip_amount\") < 0)\n",
    "tolls_amount_ = df_silver.filter(col(\"tolls_amount\") < 0)\n",
    "improvement_surcharge_ = df_silver.filter(col(\"improvement_surcharge\") < 0)\n",
    "congestion_surcharge_ = df_silver.filter(col(\"congestion_surcharge\") < 0)\n",
    "airport_fee_ = df_silver.filter(col(\"airport_fee\") < 0)\n",
    "total_amount_ = df_silver.filter(col(\"total_amount\") < 0)\n",
    "print(f\"Invalid pickup/dropoff before jan-2023: {lower_values_in.count()}/{lower_values_off.count()}\")\n",
    "print(f\"Invalid pickup/dropoff times: {dropin_.count()}\")\n",
    "print(f\"Invalid passenger count: {passenger_.count()}\")\n",
    "print(f\"Invalid trip distances: {tistances_.count()}\")\n",
    "print(f\"Invalid fares: {fate_.count()}\")\n",
    "print(f\"Invalid extras: {extra_.count()}\")\n",
    "print(f\"Invalid mta_tax: {mta_tax_.count()}\")\n",
    "print(f\"Invalid tips: {tips_.count()}\")\n",
    "print(f\"Invalid tolls_amount: {tolls_amount_.count()}\")\n",
    "print(f\"Invalid improvement_surcharge: {improvement_surcharge_.count()}\")\n",
    "print(f\"Invalid congestion_surcharge: {congestion_surcharge_.count()}\")\n",
    "print(f\"Invalid airport_fee: {airport_fee_.count()}\")\n",
    "print(f\"Invalid total_amount: {total_amount_.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4e7254-4ea2-428c-a852-f8dce725d149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Etapa 6 – Consistência Entre Campos (Camada Silver)\n",
    "\n",
    "Garantir que a integridade financeira de cada viagem seja preservada:  \n",
    "\n",
    "- **Validação do valor total**: Verificar se `total_amount` é igual à soma de seus componentes:  \n",
    "  `fare_amount + tolls_amount + tip_amount + extra + mta_tax + improvement_surcharge + congestion_surcharge + airport_fee`.  \n",
    "- **Tolerância**: Pequenas diferenças de arredondamento podem ser toleradas (ex.: dentro de ±3% de `total_amount`) para considerar pequenas discrepâncias computacionais.  \n",
    "\n",
    "Viagens que não passarem nesta verificação de consistência devem ser sinalizadas para investigação adicional ou excluídas do conjunto de dados Silver, dependendo das regras de governança de dados.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d835c76f-4738-4518-a6a3-7c42d7cae751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs as spark_abs, lit, when \n",
    "columns = [\"fare_amount\", \"tolls_amount\", \"tip_amount\", \"extra\", \"mta_tax\", \"airport_fee\", \"improvement_surcharge\", \"total_amount\"]\n",
    "df_financial = df_silver.select(*columns).fillna(0)\n",
    "df_financial = df_silver.withColumn(\"total_amount_calc\", col(\"fare_amount\")+col(\"tolls_amount\")+col(\"tip_amount\")+col(\"extra\")+col(\"mta_tax\")+col(\"airport_fee\")+col(\"improvement_surcharge\")+col(\"congestion_surcharge\")+col(\"airport_fee\"))\n",
    "tolerance_pct = 0.03\n",
    "df_financial = df_financial.withColumn(\n",
    "    \"financial_consistent\",\n",
    "    when(col(\"total_amount\") == 0, lit(0))\n",
    "    .when(\n",
    "        (spark_abs(col(\"total_amount_calc\") - col(\"total_amount\")) / col(\"total_amount\")) <= tolerance_pct,\n",
    "        lit(1)\n",
    "    ).otherwise(lit(0))\n",
    ")\n",
    "financial_inconsistencies = df_financial.filter(col(\"financial_consistent\") == 1)\n",
    "financial_consistencies = df_financial.filter(col(\"financial_consistent\") == 0)\n",
    "print(f\"Total financial inconsistencies: {financial_inconsistencies.count()} (tolerance: {100*tolerance_pct}%)\")\n",
    "print(f\"Total financial valid values: {financial_consistencies.count()} (tolerance: {100*tolerance_pct}%)\")\n",
    "print(f\"Total values: {financial_inconsistencies.count()+financial_consistencies.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5bc93d-add8-4fe4-a497-4c73ece3c697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Etapa 7 – Registros Duplicados (Camada Silver)\n",
    "\n",
    "Antes de finalizar o conjunto de dados Silver, garantir que registros de viagens duplicados sejam removidos.  \n",
    "\n",
    "- Duplicatas são identificadas com base em atributos-chave da viagem, como:  \n",
    "  `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `passenger_count` e `trip_distance`.  \n",
    "- Apenas registros únicos devem ser mantidos para garantir qualidade e confiabilidade dos dados em análises subsequentes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268ce494-4dcb-43ec-98c4-ba371ba08d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "duplicates = (\n",
    "    df_silver.groupBy(\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"passenger_count\",\"trip_distance\")\n",
    "      .agg(count(\"*\").alias(\"dup_count\"))\n",
    "      .filter(col(\"dup_count\") > 1)\n",
    ")\n",
    "print(f\"The silver dataset has {duplicates.count()} duplicated values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d47e854-b277-488e-ba81-15a58e59b56c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Planejamento – Camada Gold\n",
    "A Camada Gold fornecerá **métricas agregadas e prontas para uso empresarial** derivadas da Camada Silver.  \n",
    "O objetivo é produzir conjuntos de dados adequados para análises, relatórios e dashboards.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957e670a-d55c-4082-9500-bf24f70a94ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def remove_outliers_iqr(df: DataFrame, numeric_cols: list) -> DataFrame:\n",
    "    for c in numeric_cols:\n",
    "        quantiles = df.approxQuantile(c, [0.25, 0.75], 0.01)\n",
    "        Q1, Q3 = quantiles\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be99c831-ff13-4dae-adf4-cbff5caa1b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agregações Temporais na Camada Gold\n",
    "\n",
    "Esta função agrega métricas de viagens da Camada Silver por local de embarque e período de tempo (dia, semana ou mês), criando a base da Camada Gold.  \n",
    "\n",
    "As métricas calculadas incluem:  \n",
    "- **total_trips**: número total de viagens,  \n",
    "- **total_revenue**: receita total,  \n",
    "- **total_fare**: tarifas totais,  \n",
    "- **total_tips**: gorjetas totais,  \n",
    "- **total_tolls**: pedágios totais,  \n",
    "- **avg_distance**: distância média da viagem,  \n",
    "- **avg_trip_time**: tempo médio da viagem,  \n",
    "- **tip_percentage**: percentual médio de gorjeta em relação à receita total.  \n",
    "\n",
    "A função adiciona colunas de ano e mês, além de uma coluna de período (dia ou semana, se aplicável), permitindo análise de padrões temporais detalhados.  \n",
    "Todos os valores numéricos são arredondados para duas casas decimais para facilitar a leitura e apresentação.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88449a9-dc1e-4162-a206-2a3a75a86c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, year, month, dayofweek, hour, avg, round as spark_round\n",
    "\n",
    "# Paramters\n",
    "df = df_silver\n",
    "time_stamp = \"day\"\n",
    "location_col = [\"PUlocationID\"]\n",
    "\n",
    "# Remove outliers\n",
    "numeric_cols = [\"trip_distance\", \"total_amount\"]\n",
    "df = remove_outliers_iqr(df, numeric_cols)\n",
    "\n",
    "# Define timestamp function\n",
    "if time_stamp == \"day\":\n",
    "    stamp_func = dayofweek\n",
    "elif time_stamp == \"week\":\n",
    "    stamp_func = weekofyear\n",
    "elif time_stamp == \"month\":\n",
    "    stamp_func = month\n",
    "else:\n",
    "    raise ValueError(f\"Invalid stamp: {time_stamp}\")\n",
    "\n",
    "# Add year, month, and timestamp columns\n",
    "df = df.withColumn(\"pickup_year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "       .withColumn(\"pickup_month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
    "       .withColumn(f\"pickup_{time_stamp}\", stamp_func(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Perform aggregation\n",
    "agg_df = df.groupBy(\n",
    "    *[col(x) for x in location_col], \n",
    "    col(\"pickup_year\"), \n",
    "    col(\"pickup_month\"), \n",
    "    col(f\"pickup_{time_stamp}\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    spark_sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    spark_sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "    spark_sum(\"tip_amount\").alias(\"total_tips\"),\n",
    "    spark_sum(\"tolls_amount\").alias(\"total_tolls\"),\n",
    "    avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    avg(\"trip_time_minutes\").alias(\"avg_trip_time\"),\n",
    ")\n",
    "\n",
    "# Calculate tip percentage\n",
    "agg_df = agg_df.withColumn(\n",
    "    \"tip_percentage\",\n",
    "    when(col(\"total_revenue\") != 0, 100 * col(\"total_tips\") / col(\"total_revenue\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Round numeric columns\n",
    "agg_df = agg_df.select(\n",
    "    *location_col,\n",
    "    \"pickup_year\",\n",
    "    \"pickup_month\",\n",
    "    f\"pickup_{time_stamp}\",\n",
    "    \"total_trips\",\n",
    "    spark_round(\"total_revenue\", 2).alias(\"total_revenue\"),\n",
    "    spark_round(\"total_fare\", 2).alias(\"total_fare\"),\n",
    "    spark_round(\"total_tips\", 2).alias(\"total_tips\"),\n",
    "    spark_round(\"total_tolls\", 2).alias(\"total_tolls\"),\n",
    "    spark_round(\"avg_distance\", 2).alias(\"avg_distance\"),\n",
    "    spark_round(\"avg_trip_time\", 2).alias(\"avg_trip_time\"),\n",
    "    spark_round(\"tip_percentage\", 2).alias(\"tip_percentage\")\n",
    ")\n",
    "\n",
    "df.display()\n",
    "agg_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfdf6df-7e2f-4a46-b7f5-31a315c8abe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agregação por Faixas de Distância – Camada Gold\n",
    "\n",
    "Nesta etapa, as viagens são classificadas em **faixas de distância** para identificar padrões de viagens curtas, médias e longas, mantendo a padronização temporal com ano e mês.  \n",
    "\n",
    "A agregação é realizada por local de embarque, faixa de distância, ano, mês e período de tempo (dia, semana ou mês), calculando as seguintes métricas:  \n",
    "- **total_trips**: número total de viagens,  \n",
    "- **total_revenue**: receita total,  \n",
    "- **avg_trip_time**: tempo médio de viagem,  \n",
    "- **avg_tips**: gorjeta média.  \n",
    "\n",
    "Todos os valores numéricos são arredondados para duas casas decimais para consistência e fácil interpretação, garantindo o mesmo padrão das outras tabelas da Camada Gold.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae5d25d-1ea4-4950-a68a-ddf217d2c8da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_silver\n",
    "time_stamp = \"day\"\n",
    "location_col = []\n",
    "\n",
    "# Define faixas de distância\n",
    "df = df.withColumn(\n",
    "    \"distance_bucket\",\n",
    "    when(col(\"trip_distance\") <= 2, \"0-2 miles\")\n",
    "    .when(col(\"trip_distance\") <= 5, \"2-5 miles\")\n",
    "    .when(col(\"trip_distance\") <= 10, \"5-10 miles\")\n",
    "    .when(col(\"trip_distance\") <= 25, \"10-25 miles\")\n",
    "    .when(col(\"trip_distance\") <= 50, \"25-50 miles\")\n",
    "    .when(col(\"trip_distance\") <= 100, \"50-100 miles\")\n",
    "    .otherwise(\">100 miles\")\n",
    ")\n",
    "\n",
    "# Define função de timestamp\n",
    "if time_stamp == \"day\":\n",
    "    stamp_func = dayofweek\n",
    "elif time_stamp == \"week\":\n",
    "    stamp_func = weekofyear\n",
    "elif time_stamp == \"month\":\n",
    "    stamp_func = month\n",
    "else:\n",
    "    raise ValueError(f\"Invalid stamp: {time_stamp}\")\n",
    "\n",
    "# Adiciona colunas de ano e mês\n",
    "df = df.withColumn(\"pickup_year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "        .withColumn(\"pickup_month\", month(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Adiciona coluna de período se não for mês\n",
    "if time_stamp != \"month\":\n",
    "    df = df.withColumn(f\"pickup_{time_stamp}\", stamp_func(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Define colunas de agrupamento\n",
    "group_cols = [*location_col, \"distance_bucket\", \"pickup_year\", \"pickup_month\"]\n",
    "if time_stamp != \"month\":\n",
    "    group_cols.append(f\"pickup_{time_stamp}\")\n",
    "\n",
    "# Agregação\n",
    "agg_df = df.groupBy(*[col(c) for c in group_cols]).agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    spark_sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"trip_time_minutes\").alias(\"avg_trip_time\"),\n",
    "    avg(\"tip_amount\").alias(\"avg_tips\")\n",
    ")\n",
    "\n",
    "# Arredonda colunas numéricas\n",
    "numeric_cols = [\"total_revenue\", \"avg_trip_time\", \"avg_tips\"]\n",
    "for c in numeric_cols:\n",
    "    agg_df = agg_df.withColumn(c, spark_round(col(c), 2))\n",
    "\n",
    "agg_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea0a116-4276-47a7-b135-f67b98155973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agregação por Contagem de Passageiros – Camada Gold\n",
    "\n",
    "Esta função agrega as métricas de viagens da Camada Silver por **contagem de passageiros** e período de tempo (semana ou mês), mantendo a padronização com a coluna **ano**.  \n",
    "\n",
    "As métricas calculadas incluem:  \n",
    "- **total_trips**: número total de viagens,  \n",
    "- **total_revenue**: receita total,  \n",
    "- **avg_tips**: gorjeta média,  \n",
    "- **avg_trip_time**: tempo médio de viagem.  \n",
    "\n",
    "Todos os valores numéricos são arredondados para duas casas decimais, seguindo o padrão das tabelas Gold, garantindo consistência e legibilidade para dashboards e relatórios.  \n",
    "A função permite identificar padrões de viagens e receita conforme o tamanho do grupo de passageiros.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d70990-48c9-4a9d-b3f3-658cb1c8a30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_silver\n",
    "time_stamp = \"day\"\n",
    "location_col = []\n",
    "\n",
    "# Define função de timestamp\n",
    "if time_stamp == \"day\":\n",
    "    stamp_func = dayofweek\n",
    "elif time_stamp == \"week\":\n",
    "    stamp_func = weekofyear\n",
    "elif time_stamp == \"month\":\n",
    "    stamp_func = month\n",
    "else:\n",
    "    raise ValueError(f\"Invalid stamp: {time_stamp}\")\n",
    "\n",
    "# Adiciona colunas de ano e mês\n",
    "df = df.withColumn(\"pickup_year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "        .withColumn(\"pickup_month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
    "        .withColumn(f\"pickup_{time_stamp}\", stamp_func(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Define colunas de agrupamento\n",
    "group_cols = [*location_col, \"passenger_count\", \"pickup_year\", \"pickup_month\", f\"pickup_{time_stamp}\"]\n",
    "\n",
    "# Agregação\n",
    "agg_df = df.groupBy(*[col(c) for c in group_cols]).agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    spark_sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"tip_amount\").alias(\"avg_tips\"),\n",
    "    avg(\"trip_time_minutes\").alias(\"avg_trip_time\")\n",
    ")\n",
    "\n",
    "# Arredonda colunas numéricas\n",
    "numeric_cols = [\"total_revenue\", \"avg_tips\", \"avg_trip_time\"]\n",
    "for c in numeric_cols:\n",
    "    agg_df = agg_df.withColumn(c, spark_round(col(c), 2))\n",
    "\n",
    "agg_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_exploration_silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
